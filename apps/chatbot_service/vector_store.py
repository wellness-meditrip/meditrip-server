"""
vector_store.py - Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸
ë¬¸ì„œ ì„ë² ë”© ì €ì¥ ë° ìœ ì‚¬ë„ ê²€ìƒ‰ ê´€ë¦¬
"""

import os
import logging
from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from langchain.embeddings import OpenAIEmbeddings

logger = logging.getLogger(__name__)


class QdrantVectorStore:
    """Qdrant ë²¡í„° ì €ì¥ì†Œ í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.qdrant_url = os.getenv("QDRANT_URL", "http://localhost:6333")
        self.collection_name = "medical_documents"
        self.client = None
        self.embeddings = None
        self._initialize_client()
        self._initialize_embeddings()
    
    def _initialize_client(self):
        """Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            self.client = QdrantClient(url=self.qdrant_url)
            logger.info(f"âœ… Qdrant ì—°ê²° ì„±ê³µ: {self.qdrant_url}")
        except Exception as e:
            logger.error(f"âŒ Qdrant ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def _initialize_embeddings(self):
        """OpenAI ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=openai_api_key,
                model="text-embedding-ada-002"
            )
            logger.info("âœ… OpenAI ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ OpenAI ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def create_collection(self, vector_size: int = 1536):
        """ì»¬ë ‰ì…˜ ìƒì„± (OpenAI ada-002ëŠ” 1536 ì°¨ì›)"""
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸
            collections = self.client.get_collections()
            collection_names = [c.name for c in collections.collections]
            
            if self.collection_name in collection_names:
                logger.info(f"ğŸ“‹ ì»¬ë ‰ì…˜ '{self.collection_name}' ì´ë¯¸ ì¡´ì¬í•¨")
                return True
            
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=Distance.COSINE
                )
            )
            logger.info(f"âœ… ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def add_documents(self, texts: List[str], metadatas: List[Dict[str, Any]]):
        """ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)"""
        try:
            if len(texts) != len(metadatas):
                raise ValueError("í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            logger.info(f"ğŸ“„ {len(texts)}ê°œ ë¬¸ì„œ ë°°ì¹˜ ì„ë² ë”© ì‹œì‘...")
            
            # ë°°ì¹˜ í¬ê¸° ì„¤ì • (OpenAI í† í° ì œí•œ ê³ ë ¤)
            batch_size = 50  # í•œ ë²ˆì— 50ê°œì”© ì²˜ë¦¬
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            all_points = []
            point_id = 0
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(texts))
                
                batch_texts = texts[start_idx:end_idx]
                batch_metadatas = metadatas[start_idx:end_idx]
                
                logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_texts)}ê°œ ë¬¸ì„œ)")
                
                try:
                    # ë°°ì¹˜ë³„ ì„ë² ë”© ìƒì„±
                    batch_embeddings = self.embeddings.embed_documents(batch_texts)
                    
                    # í¬ì¸íŠ¸ ìƒì„±
                    for text, metadata, vector in zip(batch_texts, batch_metadatas, batch_embeddings):
                        point = PointStruct(
                            id=point_id,
                            vector=vector,
                            payload={
                                "text": text,
                                "page": metadata.get("page", 0),
                                "source": metadata.get("source", "unknown"),
                                **metadata
                            }
                        )
                        all_points.append(point)
                        point_id += 1
                    
                    logger.info(f"âœ… ë°°ì¹˜ {batch_idx + 1} ì„ë² ë”© ì™„ë£Œ")
                    
                except Exception as batch_error:
                    logger.error(f"âŒ ë°°ì¹˜ {batch_idx + 1} ì‹¤íŒ¨: {batch_error}")
                    # ë°°ì¹˜ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    continue
            
            if not all_points:
                raise Exception("ëª¨ë“  ë°°ì¹˜ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            # Qdrantì— ì €ì¥ (ë°°ì¹˜ ì—…ë¡œë“œ)
            logger.info(f"ğŸ’¾ Qdrantì— {len(all_points)}ê°œ ë¬¸ì„œ ì €ì¥ ì¤‘...")
            
            # Qdrantë„ ë°°ì¹˜ í¬ê¸° ì œí•œ
            upload_batch_size = 100
            for i in range(0, len(all_points), upload_batch_size):
                batch_points = all_points[i:i + upload_batch_size]
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=batch_points
                )
                logger.info(f"ğŸ“¦ {i + len(batch_points)}/{len(all_points)} ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")
            
            logger.info(f"ğŸ‰ ì´ {len(all_points)}ê°œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def search_similar(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_vector = self.embeddings.embed_query(query)
            
            # Qdrantì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_vector,
                limit=limit,
                with_payload=True
            )
            
            # ê²°ê³¼ ì •ë¦¬
            results = []
            for result in search_results:
                results.append({
                    "text": result.payload.get("text", ""),
                    "page": result.payload.get("page", 0),
                    "source": result.payload.get("source", "unknown"),
                    "score": result.score,
                    "metadata": result.payload
                })
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì„œ ë°œê²¬")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_collection_info(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            info = self.client.get_collection(self.collection_name)
            return {
                "name": self.collection_name,
                "points_count": info.points_count,
                "status": info.status,
                "vector_size": info.config.params.vectors.size,
                "distance": info.config.params.vectors.distance
            }
        except Exception as e:
            logger.error(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def health_check(self) -> bool:
        """Qdrant ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            self.client.get_collections()
            return True
        except Exception as e:
            logger.error(f"âŒ Qdrant í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return False